#!/usr/bin/env python3
"""
XSS Vibes V2 - Mass Endpoint Validator
Validate discovered endpoints for accessibility and responsiveness
"""

import asyncio
import aiohttp
import json
import logging
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any, Set
from datetime import datetime
import ssl
from urllib.parse import urlparse, urljoin
import re


class EndpointValidator:
    def __init__(self, max_concurrent: int = 50, timeout: int = 10):
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.setup_logging()
        self.session = None

    def setup_logging(self):
        """Setup logging configuration"""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("mass_validator.log"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger("MassValidator")

    async def create_session(self):
        """Create aiohttp session with proper configuration"""
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            ssl=ssl.create_default_context(),
            verify_ssl=False,  # For testing purposes
            enable_cleanup_closed=True,
        )

        headers = {
            "User-Agent": "XSS-Vibes-V2-Validator/1.0 (Security Research)",
            "Accept": "*/*",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

        timeout = aiohttp.ClientTimeout(total=self.timeout)

        self.session = aiohttp.ClientSession(
            connector=connector, headers=headers, timeout=timeout
        )

    async def close_session(self):
        """Close aiohttp session"""
        if self.session:
            await self.session.close()

    async def validate_endpoint(self, url: str) -> Dict[str, Any]:
        """Validate single endpoint"""
        start_time = time.time()
        result = {
            "url": url,
            "status": "unknown",
            "status_code": None,
            "response_time": None,
            "content_length": None,
            "content_type": None,
            "server": None,
            "title": None,
            "technologies": [],
            "security_headers": {},
            "interesting_paths": [],
            "error": None,
            "timestamp": datetime.now().isoformat(),
        }

        try:
            if not self.session:
                await self.create_session()

            async with self.session.get(url) as response:
                result["status_code"] = response.status
                result["response_time"] = round((time.time() - start_time) * 1000, 2)
                result["content_type"] = response.headers.get("content-type", "")
                result["server"] = response.headers.get("server", "")

                # Security headers analysis
                security_headers = {
                    "x-frame-options": response.headers.get("x-frame-options"),
                    "x-content-type-options": response.headers.get(
                        "x-content-type-options"
                    ),
                    "x-xss-protection": response.headers.get("x-xss-protection"),
                    "strict-transport-security": response.headers.get(
                        "strict-transport-security"
                    ),
                    "content-security-policy": response.headers.get(
                        "content-security-policy"
                    ),
                    "access-control-allow-origin": response.headers.get(
                        "access-control-allow-origin"
                    ),
                }
                result["security_headers"] = {
                    k: v for k, v in security_headers.items() if v
                }

                # Read content for analysis
                try:
                    content = await response.text(encoding="utf-8", errors="ignore")
                    result["content_length"] = len(content)

                    # Extract title
                    title_match = re.search(
                        r"<title[^>]*>([^<]+)</title>", content, re.IGNORECASE
                    )
                    if title_match:
                        result["title"] = title_match.group(1).strip()

                    # Technology detection
                    result["technologies"] = self.detect_technologies(
                        content, response.headers
                    )

                    # Find interesting paths
                    result["interesting_paths"] = self.find_interesting_paths(
                        content, url
                    )

                except Exception as e:
                    result["error"] = f"Content reading error: {str(e)}"

                # Determine status
                if response.status == 200:
                    result["status"] = "accessible"
                elif response.status in [301, 302, 307, 308]:
                    result["status"] = "redirect"
                elif response.status == 401:
                    result["status"] = "auth_required"
                elif response.status == 403:
                    result["status"] = "forbidden"
                elif response.status == 404:
                    result["status"] = "not_found"
                elif response.status >= 500:
                    result["status"] = "server_error"
                else:
                    result["status"] = "other"

        except asyncio.TimeoutError:
            result["status"] = "timeout"
            result["error"] = "Request timeout"
        except aiohttp.ClientConnectorError as e:
            result["status"] = "connection_error"
            result["error"] = f"Connection error: {str(e)}"
        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)

        return result

    def detect_technologies(self, content: str, headers: Any) -> List[str]:
        """Detect technologies from content and headers"""
        technologies = []

        # Server detection
        server = headers.get("server", "").lower()
        if "nginx" in server:
            technologies.append("nginx")
        if "apache" in server:
            technologies.append("apache")
        if "iis" in server:
            technologies.append("iis")

        # Framework detection from content
        content_lower = content.lower()

        # JavaScript frameworks
        if "react" in content_lower or "data-reactroot" in content_lower:
            technologies.append("react")
        if "angular" in content_lower or "ng-" in content_lower:
            technologies.append("angular")
        if "vue" in content_lower or "v-" in content_lower:
            technologies.append("vue")
        if "jquery" in content_lower:
            technologies.append("jquery")

        # Backend frameworks
        if "laravel" in content_lower or "laravel_session" in content_lower:
            technologies.append("laravel")
        if "django" in content_lower or "csrftoken" in content_lower:
            technologies.append("django")
        if "rails" in content_lower or "csrf-token" in content_lower:
            technologies.append("rails")
        if "spring" in content_lower:
            technologies.append("spring")
        if "express" in content_lower:
            technologies.append("express")

        # CMS detection
        if "wordpress" in content_lower or "wp-content" in content_lower:
            technologies.append("wordpress")
        if "drupal" in content_lower:
            technologies.append("drupal")
        if "joomla" in content_lower:
            technologies.append("joomla")

        # Admin panels
        if "phpmyadmin" in content_lower:
            technologies.append("phpmyadmin")
        if "adminer" in content_lower:
            technologies.append("adminer")

        return technologies

    def find_interesting_paths(self, content: str, base_url: str) -> List[str]:
        """Find interesting paths in content"""
        interesting = []

        # Common patterns for interesting endpoints
        patterns = [
            r'/api/[^"\s]+',
            r'/admin/[^"\s]+',
            r'/dashboard/[^"\s]+',
            r'/management/[^"\s]+',
            r'/debug/[^"\s]+',
            r'/test/[^"\s]+',
            r'/dev/[^"\s]+',
            r'\.php[^"\s]*',
            r'\.json[^"\s]*',
            r'\.xml[^"\s]*',
            r'/soa2/[^"\s]+',
            r'/restapi/[^"\s]+',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                # Clean up and make absolute URL
                path = match.strip("\"'")
                if path.startswith("/"):
                    full_url = urljoin(base_url, path)
                    interesting.append(full_url)

        return list(set(interesting))  # Remove duplicates

    async def validate_endpoints_batch(
        self, endpoints: List[str]
    ) -> List[Dict[str, Any]]:
        """Validate endpoints in batches"""
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def validate_with_semaphore(url: str):
            async with semaphore:
                return await self.validate_endpoint(url)

        self.logger.info(
            f"üîç Validating {len(endpoints)} endpoints with {self.max_concurrent} concurrent requests"
        )

        tasks = [validate_with_semaphore(endpoint) for endpoint in endpoints]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Filter out exceptions
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"Error validating {endpoints[i]}: {result}")
                # Create error result
                error_result = {
                    "url": endpoints[i],
                    "status": "error",
                    "error": str(result),
                    "timestamp": datetime.now().isoformat(),
                }
                valid_results.append(error_result)
            else:
                valid_results.append(result)

        return valid_results

    def load_endpoints_from_file(self, filename: str) -> List[str]:
        """Load endpoints from various file formats"""
        endpoints = []

        try:
            if filename.endswith(".json"):
                with open(filename, "r") as f:
                    data = json.load(f)

                # Handle different JSON formats
                if isinstance(data, list):
                    endpoints = data
                elif "targets" in data:
                    endpoints = data["targets"]
                elif "fofa_targets" in data:
                    endpoints.extend(data["fofa_targets"])
                    if "shodan_targets" in data:
                        endpoints.extend(data["shodan_targets"])
                elif "unique_targets" in data:
                    endpoints = data["unique_targets"]

            else:
                # Plain text file
                with open(filename, "r") as f:
                    endpoints = [
                        line.strip()
                        for line in f
                        if line.strip() and not line.startswith("#")
                    ]

        except Exception as e:
            self.logger.error(f"Error loading endpoints from {filename}: {e}")

        # Ensure URLs have protocol
        normalized_endpoints = []
        for endpoint in endpoints:
            if not endpoint.startswith(("http://", "https://")):
                # Try both protocols
                normalized_endpoints.extend(
                    [f"http://{endpoint}", f"https://{endpoint}"]
                )
            else:
                normalized_endpoints.append(endpoint)

        return list(set(normalized_endpoints))  # Remove duplicates

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze validation results"""
        analysis = {
            "total_endpoints": len(results),
            "status_breakdown": {},
            "technologies": {},
            "response_times": {"fast": 0, "medium": 0, "slow": 0},
            "security_analysis": {"missing_headers": 0, "weak_headers": 0},
            "interesting_findings": [],
            "accessible_endpoints": [],
            "failed_endpoints": [],
        }

        for result in results:
            status = result.get("status", "unknown")
            analysis["status_breakdown"][status] = (
                analysis["status_breakdown"].get(status, 0) + 1
            )

            # Response time analysis
            response_time = result.get("response_time", 0)
            if response_time < 500:
                analysis["response_times"]["fast"] += 1
            elif response_time < 2000:
                analysis["response_times"]["medium"] += 1
            else:
                analysis["response_times"]["slow"] += 1

            # Technology analysis
            for tech in result.get("technologies", []):
                analysis["technologies"][tech] = (
                    analysis["technologies"].get(tech, 0) + 1
                )

            # Security headers analysis
            headers = result.get("security_headers", {})
            if not headers:
                analysis["security_analysis"]["missing_headers"] += 1

            # Collect accessible endpoints
            if status == "accessible":
                analysis["accessible_endpoints"].append(
                    {
                        "url": result["url"],
                        "title": result.get("title"),
                        "technologies": result.get("technologies", []),
                        "response_time": result.get("response_time"),
                    }
                )
            elif status in ["error", "timeout", "connection_error"]:
                analysis["failed_endpoints"].append(
                    {"url": result["url"], "error": result.get("error")}
                )

            # Interesting findings
            if result.get("interesting_paths"):
                analysis["interesting_findings"].append(
                    {"url": result["url"], "paths": result["interesting_paths"]}
                )

        return analysis

    def save_results(self, results: List[Dict[str, Any]], filename: str):
        """Save validation results"""
        try:
            with open(filename, "w") as f:
                json.dump(results, f, indent=2)
            self.logger.info(f"üìÅ Results saved to: {filename}")
        except Exception as e:
            self.logger.error(f"Error saving results: {e}")

    def print_summary(self, analysis: Dict[str, Any]):
        """Print validation summary"""
        print("\nüéØ Mass Validation Summary")
        print("=" * 50)
        print(f"üìä Total Endpoints: {analysis['total_endpoints']}")

        print("\nüìà Status Breakdown:")
        for status, count in analysis["status_breakdown"].items():
            percentage = (count / analysis["total_endpoints"]) * 100
            print(f"   {status}: {count} ({percentage:.1f}%)")

        print(f"\nüöÄ Performance:")
        rt = analysis["response_times"]
        print(f"   Fast (<500ms): {rt['fast']}")
        print(f"   Medium (500ms-2s): {rt['medium']}")
        print(f"   Slow (>2s): {rt['slow']}")

        if analysis["technologies"]:
            print(f"\nüîß Technologies Found:")
            for tech, count in sorted(
                analysis["technologies"].items(), key=lambda x: x[1], reverse=True
            )[:10]:
                print(f"   {tech}: {count}")

        accessible = analysis["accessible_endpoints"][:10]
        if accessible:
            print(f"\n‚úÖ Top Accessible Endpoints:")
            for i, endpoint in enumerate(accessible, 1):
                title = endpoint.get("title", "No title")[:50]
                time_ms = endpoint.get("response_time", 0)
                print(f"   {i}. {endpoint['url']} ({time_ms}ms) - {title}")

        interesting = analysis["interesting_findings"][:5]
        if interesting:
            print(f"\nüîç Interesting Findings:")
            for finding in interesting:
                print(
                    f"   {finding['url']} - {len(finding['paths'])} interesting paths"
                )


async def main():
    parser = argparse.ArgumentParser(
        description="XSS Vibes V2 - Mass Endpoint Validator"
    )
    parser.add_argument("-f", "--file", help="File containing endpoints to validate")
    parser.add_argument(
        "-o",
        "--output",
        default="validation_results.json",
        help="Output file for results",
    )
    parser.add_argument(
        "-c", "--concurrent", type=int, default=50, help="Maximum concurrent requests"
    )
    parser.add_argument(
        "-t", "--timeout", type=int, default=10, help="Request timeout in seconds"
    )
    parser.add_argument("--analyze-only", help="Analyze existing results file")

    args = parser.parse_args()

    print("üéØ XSS Vibes V2 - Mass Endpoint Validator")
    print("=" * 50)

    validator = EndpointValidator(max_concurrent=args.concurrent, timeout=args.timeout)

    try:
        if args.analyze_only:
            # Just analyze existing results
            with open(args.analyze_only, "r") as f:
                results = json.load(f)
            analysis = validator.analyze_results(results)
            validator.print_summary(analysis)
            return

        # Load endpoints
        if args.file:
            endpoints = validator.load_endpoints_from_file(args.file)
        else:
            # Try to load from recent discovery results
            discovery_files = [
                "simple_target_hunt.json",
                "fofa_results.json",
                "shodan_results.json",
            ]
            endpoints = []
            for file in discovery_files:
                if Path(file).exists():
                    endpoints = validator.load_endpoints_from_file(file)
                    print(f"üìÇ Loaded endpoints from: {file}")
                    break

        if not endpoints:
            print("‚ùå No endpoints found. Run discovery first or specify input file.")
            return

        print(f"üìä Loaded {len(endpoints)} endpoints for validation")

        # Create session and validate
        await validator.create_session()
        results = await validator.validate_endpoints_batch(endpoints)
        await validator.close_session()

        # Analyze and save results
        analysis = validator.analyze_results(results)
        validator.save_results(results, args.output)
        validator.print_summary(analysis)

        print(f"\nüöÄ Validation complete! Results saved to: {args.output}")

    except Exception as e:
        validator.logger.error(f"Validation failed: {e}")
        print(f"‚ùå Error: {e}")
    finally:
        await validator.close_session()


if __name__ == "__main__":
    asyncio.run(main())
